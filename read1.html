<doctype HTML>
<head>
<title>Reading Sample 1</title>
<link rel="stylesheet" type="text/css" href="styles/btn.css">
<meta name="viewport" content="width=device-width, initial-scale=1">

<style>
.highlight
{
background-color:yellow;
}
p
{
padding-left: 30px;
max-width: 75%
}
</style>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
<script>
jQuery(function(){
    
	jQuery("p,h1,h2,h3,h4,li,.para").bind('click',function(){
    //jQuery('.para').bind('click',function(){
           
           var elm=jQuery(this);
           if(elm.hasClass('highlight'))
           {
              elm.removeClass('highlight')
           }
           else
           { 
                 jQuery('.highlight').removeClass('highlight');
                 elm.addClass('highlight');
           }
   });
    
});
</script>

</head>
<body>
<!--Citation -->
<h3>Citation</h3>
<p>
A. Akusok, K. M. Bj√∂rk, Y. Miche and A. Lendasse, "High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications," in IEEE Access, vol. 3, no. , pp. 1011-1025, 2015.
doi: 10.1109/ACCESS.2015.2450498
URL: http://ieeexplore.ieee.org.proxy-um.researchport.umd.edu/stamp/stamp.jsp?tp=&arnumber=7140733&isnumber=7042252
</p>

<!-- Article begins-->

<h1>High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications</h1>
<p>
This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.
</p>
<p>
This paper appears in: Access, IEEE, Issue Date: 2015, Written by: Akusok, A.; Bjork, K.-M.; Miche, Y.; Lendasse, A.
</p>

<div class="kicker">SECTION I</div>
<h2>I<small>NTRODUCTION</small></h2>
</div>
<p><a id="note-ref_1a"></a> <a id="note-ref_1b"></a> <a id="note-ref_2a"></a> <a id="note-ref_3a"></a> <a id="note-ref_3b"></a> <a id="note-ref_4a"></a> <a id="note-ref_5a"></a> <a id="note-ref_6a"></a> <a id="note-ref_6b"></a> <a id="note-ref_7a"></a>Extreme Learning Machines [<a href="#ref_1">1</a>]&#x2013;[<a href="#ref_2">2</a>] [<a href="#ref_3">3</a>][<a href="#ref_4">4</a>] (ELM) as<a href="#fn_1"><sup>1</sup></a> important emergent machine learning techniques, are proposed for both &#x201C;generalized&#x201D; Single-Layer Feed-forward Networks (SLFNs) [<a href="#ref_1">1</a>], [<a href="#ref_3">3</a>], [<a href="#ref_5">5</a>]&#x2013;[<a href="#ref_6">6</a>][<a href="#ref_7">7</a>] and multi layered feedforward networks [<a href="#ref_6">6</a>].<a id="note-ref_5b"></a> <a id="note-ref_6c"></a> <a id="note-ref_7b"></a> Unlike traditional learning theories and learning algorithms, ELM theories show that hidden neurons need not be tuned in learning and their parameters can be independent of the training data, but nevertheless ELMs have universal approximation and classification properties [<a href="#ref_5">5</a>]&#x2013;[<a href="#ref_6">6</a>][<a href="#ref_7">7</a>]. In most cases, the ELM hidden neurons can be randomly generated, which means that all the parameters of the hidden neurons (e.g., the input weights and biases of additive neurons, the centres and the impact factors of RBF nodes, frequencies and the shift of Fourier series, etc) can be randomly generated and therefore also independent of the training data.<a id="note-ref_8a"></a> <a id="note-ref_9a"></a> <a id="note-ref_10a"></a> <a id="note-ref_11a"></a> <a id="note-ref_12a"></a> Some related efforts had been attempted before [<a href="#ref_8">8</a>]&#x2013;[<a href="#ref_9">9</a>][<a href="#ref_10">10</a>] with parts of SLFN generated randomly or taken from a subset of data samples [<a href="#ref_11">11</a>], however, they either lack proof of the universal approximation capability for fully randomized hidden neurons, or can be considered as specific cases of ELM [<a href="#ref_12">12</a>].</p>
<p><a id="note-ref_6d"></a> <a id="note-ref_7c"></a>ELM, consisting of a wide type of feed forward neural networks, is the first method [<a href="#ref_6">6</a>], [<a href="#ref_7">7</a>], which can universally approximate any continuous function with almost any nonlinear and piecewise continuous hidden neurons.</p>
<p><a id="note-ref_13a"></a>A distinct property of ELM is the non-iterative linear solution for the output weights, which is possible because there is no dependence between the input and output weights like in the Back-propagation [<a href="#ref_13">13</a>] training procedure.<a id="note-ref_14a"></a> <a id="note-ref_15a"></a> A non-iterative solution of ELMs provides a speedup of 5 orders of magnitude compared to Multilayer Perceptron [<a href="#ref_14">14</a>] (MLP) or 6 orders of magnitude compared to Support Vector Machines [<a href="#ref_15">15</a>] (SVM), as shown in the experimental section.</p>
<p><a id="note-ref_1c"></a> <a id="note-ref_16a"></a>ELM originally belongs to the set of regression methods [<a href="#ref_1">1</a>], [<a href="#ref_16">16</a>]. The universal approximation property implies that an ELM can solve any regression problem with a desired accuracy, if it has enough hidden neurons and training data to learn parameters for all the hidden neurons.<a id="note-ref_3c"></a> ELMs are also easily adapted for classification problems [<a href="#ref_3">3</a>]. For multiclass classification, the index of the output node with the highest output indicates the predicted label of input. Then the predicted class is assigned by the maximum output of an ELM.<a id="note-ref_17a"></a> Multi-label classification [<a href="#ref_17">17</a>] is handled similarly, but the predicted classes are assigned by all outputs, which are greater than some threshold value.</p>
<p><a id="note-ref_18a"></a>Extreme Learning Machines are well suited for solving Big Data [<a href="#ref_18">18</a>] problems because their solution is so rapidly obtained.<a id="note-ref_19a"></a> <a id="note-ref_20a"></a> <a id="note-ref_21a"></a> <a id="note-ref_22a"></a> Indeed, they are used for analyzing Big Data [<a href="#ref_19">19</a>]&#x2013;[<a href="#ref_20">20</a>] [<a href="#ref_21">21</a>][<a href="#ref_22">22</a>].<a id="note-ref_23a"></a> <a id="note-ref_24a"></a> But only two ELM toolboxes [<a href="#ref_23">23</a>], [<a href="#ref_24">24</a>] of all<a href="#fn_2"><sup>2</sup></a> available can process a dataset larger than a given computer memory, and they both implement a particular method rather than focus on overall ELM performance.<a id="note-ref_25a"></a> <a id="note-ref_26a"></a> A GPU acceleration [<a href="#ref_25">25</a>], [<a href="#ref_26">26</a>] speeds up the computation significantly, but there is no ready to use implementation before the current work in this article.</p>
<p>Extreme Learning Machines also benefit greatly from model structure selection and regularization, which reduces the negative effects of random initialization and over-fitting.<a id="note-ref_27a"></a> <a id="note-ref_28a"></a> <a id="note-ref_29a"></a> <a id="note-ref_30a"></a> <a id="note-ref_31a"></a> The methods include <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-1-small.png" width="16" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{1}$</span></span> [<a href="#ref_27">27</a>], [<a href="#ref_28">28</a>] and <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-2-small.png" width="17" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{2}$</span></span> [<a href="#ref_29">29</a>] regularization, as well as other methods [<a href="#ref_30">30</a>] like handling imbalance classification [<a href="#ref_31">31</a>].<a id="note-ref_28b"></a> The problem is again the absence of ready to use toolboxes, which are focused on particular existing methods [<a href="#ref_28">28</a>]. One reason for this is found in the fact that these methods are challenging to implement since they are typically computationally intensive and are not well suitable for Big Data.</p>
<p>The goal of this work is to approach the vast field of Extreme Learning Machines from a practical performance point of view, and to provide an efficient and easy toolbox, which saves time of researchers and data analysts desiring to apply ELM to their existing problems. An analysis of training methods is done in this piece of software, to select the fastest, least bounded by memory, scalable and simplest way of training ELMs. An efficient implementation is created which suits even old machines of low performance, and the software also handles Big Data on modern workstations with accelerators. The proposed toolbox includes all major model structure selection options and regularization methods, tools for Big Data pre-processing and parallel computing. In the next two sections we explain theoretical and practical aspects of the ELMs methodology. <a href="#sec4">Section IV</a> explains the actual ELM toolbox, and <a href="#sec5">section V</a> compares and discusses on the toolbox performance on various datasets, including test sets of Big Data.</p>
</div>
<div class="section" id="sec2">
<div class="header article-hdr">
<div class="kicker">SECTION II</div>
<h2>E<small>XTREME</small> L<small>EARNING</small> M<small>ACHINES</small> M<small>ETHODOLOGY</small></h2>
</div>
<div class="section_2" id="sec2a">
<h3>A. ELM Model</h3>
<p>An ELM is a fast training method for SLFN networks (<a href="#fig_1">Figure 1</a>). A SLFN has three layers of neurons, but the name <i>Single</i> comes from the only layer of non-linear neurons in the model: the hidden layer. Input layer provides data features and performs no computations, while an output layer is linear without a transformation function and without bias.</p>
<div class="figure figure-full" id="fig_1">
<div class="img-wrap">
<a href="/ielx7/6287639/7042252/7140733/html/img/7140733-fig-1-large.gif"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-fig-1-small.gif" width="549" height="493" alt="Figure 1" /></a>
</div>
<div class="figcaption">
<b class="title">Figure 1.</b> Computing the output of an SLFN (ELM) model.<!--&#x00A9; 2015 IEEE--></div>
<p class="links"><a href="icp.jsp?arnumber=7140733&pgName=figures" class="all">View All</a> | <a href="#fig_2" class="next">Next</a></p>
</div>
<p>In the ELM method, input layer weights <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-3-small.png" width="19" height="13" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {W}}$</span></span> and biases <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-4-small.png" width="10" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$\mathbf {b}$</span></span> are set randomly and never adjusted (random distribution of the weights is discussed in <a href="#sec3a">section III-A</a>).<a id="note-ref_13b"></a> Because the input weights are fixed, the output weights <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-5-small.png" width="11" height="16" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\boldsymbol {\beta }}$</span></span> are independent of them (unlike in Back-propagation [<a href="#ref_13">13</a>] training method) and have a direct solution without iteration. For a linear output layer, such solution is also linear and very fast to compute.</p>
<p>Random input layer weights improve the generalization properties of the solution of a linear output layer, because they produce almost orthogonal (weakly correlated) hidden layer features. The solution of a linear system is always in a span of inputs. If the range of solution weights is limited, orthogonal inputs provide a larger solution space volume with these constrained weights. Small norms of the weights tend to make the system more stable and noise resistant as errors in input will not be amplified in the output of the linear system with smaller coefficients. Thus random hidden layer generates weakly correlated hidden layer features, which allow for a solution with a small norm and a good generalization performance.</p>
<p>A formal description of an ELM is following. Consider a set of <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-6-small.png" width="15" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$N$</span></span> distinct training samples <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-7-small.png" width="126" height="19" alt="Formula" /><span class="html-formula"></span><span class="tex">$( {\mathbf {x}}_{i}, {\mathbf {t}}_{i}), ~i \in [\![ 1, N ]\!]$</span></span> with <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-8-small.png" width="53" height="17" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {x}}_{i} \in \mathbb {R}^{d}$</span></span> and <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-9-small.png" width="50" height="15" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {t}}_{i} \in \mathbb {R}^{c}$</span></span>.<a id="note-ref_3d"></a> <a id="note-ref_6e"></a> <a id="note-ref_7d"></a> Then a SLFN with <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-10-small.png" width="11" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$L$</span></span> hidden neurons has the following output equation:<span class="formula" id="deqn1"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqdisp-1-small.png" width="336" height="62" alt="Formula" /><span class="link">TeX Source</span><span class="tex">\begin{equation} \sum ^{L}_{j=1} {\boldsymbol {\beta }}_{j} \phi ( {\mathbf {w}}_{j} {\mathbf {x}}_{i} + b_{j}), ~i \in [\![ 1, N ]\!], \end{equation}</span></span> with <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-11-small.png" width="10" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$\phi $</span></span> being the activation function (a sigmoid function is a common choice, but other activation functions are possible including linear) [<a href="#ref_3">3</a>], [<a href="#ref_6">6</a>], [<a href="#ref_7">7</a>], <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-12-small.png" width="17" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {w}}_{i}$</span></span> the input weights, <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-13-small.png" width="11" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$b_{i}$</span></span> the biases and <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-14-small.png" width="15" height="16" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\boldsymbol {\beta }}_{i}$</span></span> the output weights.</p>
<p>The relation between inputs <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-15-small.png" width="14" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {x}}_{i}$</span></span> of the network, target outputs <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-16-small.png" width="11" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {t}}_{i}$</span></span> and estimated outputs <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-17-small.png" width="14" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {y}}_{i}$</span></span> is:<span class="formula" id="deqn2"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqdisp-2-small.png" width="380" height="59" alt="Formula" /><span class="link">TeX Source</span><span class="tex">\begin{equation} {\mathbf {y}}_{i} = \sum ^{L}_{j=1} {\boldsymbol {\beta }}_{j} \phi ( {\mathbf {w}}_{j} {\mathbf {x}}_{i} + b_{j}) = {\mathbf {t}}_{i} + \epsilon _{i}, ~i \in [\![ 1, N ]\!], \end{equation}</span></span> where <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-18-small.png" width="7" height="7" alt="Formula" /><span class="html-formula"></span><span class="tex">$\epsilon $</span></span> is noise. Here the <i>noise</i> includes both random noise and dependency on variables not presented in the inputs <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-19-small.png" width="14" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {X}}$</span></span>.</p>
</div>
<div class="section_2" id="sec2b">
<h3>B. Hidden Neurons</h3>
<p>Hidden neurons transform the input data into a different representation. The transformation is done in two steps. First, the data is projected into the hidden layer using the input layer weights and biases. Second, the projected data is transformed. A non-linear transformation function greatly increases the learning capabilities of an ELM, because it is the only place where a non-linear part can be added in ELM method. After transformation, the data in the hidden layer representation <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-20-small.png" width="10" height="13" alt="Formula" /><span class="html-formula"></span><span class="tex">$\mathbf {h}$</span></span> (see <a href="#fig_1">Figure 1</a>) is used for finding output layer weights.</p>
<p>The hidden layer is not constrained to have only one type of transformation function in neurons.<a id="note-ref_3e"></a> <a id="note-ref_6f"></a> <a id="note-ref_7e"></a> Different functions can be used (sigmoid, hyperbolic tangent, threshold, etc.) [<a href="#ref_3">3</a>], [<a href="#ref_6">6</a>], [<a href="#ref_7">7</a>]. Some neurons may have no transformation function at all. They are linear neurons, and learn linear dependencies between data features and targets directly, without approximating them by a non-linear function. Usually the number of linear neurons equals the number of data features, and each of these neurons copy the corresponding feature (by using an identity <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-21-small.png" width="19" height="13" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {W}}$</span></span> and zero <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-22-small.png" width="10" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$\mathbf {b}$</span></span>).</p>
<p><a id="note-ref_32a"></a>Another type of neurons commonly present in ELMs is the Radial Basis Function (RBF) neurons [<a href="#ref_32">32</a>]. They use distances to centroids as inputs to the hidden layer, instead of a linear projections. The non-linear projection function is applied as usual. ELMs with RBF neurons compute predictions based on similar training data samples, which helps solving tasks with a complex dependency between data features and targets. Any function (norm) of distances between samples and centroids can be used, for instance <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-23-small.png" width="17" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{2}$</span></span>, <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-24-small.png" width="16" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{1}$</span></span> or <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-25-small.png" width="22" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^\infty $</span></span> norms.</p>
</div>
<div class="section_2" id="sec2c">
<h3>C. Matrix Form of ELMs</h3>
<p>Practically, ELMs are often solved in a matrix form by a closed form solution. An implementation with matrices is easy to write and fast to run on computers. An ELM is written in a matrix form by gathering outputs of all hidden neurons into a matrix <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-26-small.png" width="14" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}$</span></span> as on <a href="#deqn3-4">equation 3</a>. A graphical representation is shown in <a href="#fig_2">Figure 2</a>. The matrix form of ELMs is used in the paper hereafter.<span class="formula" id="deqn3-4"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqdisp-3-small.png" width="369" height="122" alt="Formula" /><span class="link">TeX Source</span><span class="tex">\begin{align} \mathbf {H}=&amp;\begin{bmatrix} \phi ( {\mathbf {w}}_{1} {\mathbf {x}}_{1} + b_{1}) &amp; \cdots &amp; \phi ( {\mathbf {w}}_{L} {\mathbf {x}}_{1} + b_{L})\\ \vdots &amp; \ddots &amp; \vdots \\ \phi ( {\mathbf {w}}_{1} {\mathbf {x}}_{N} + b_{1}) &amp; \cdots &amp; \phi ( {\mathbf {w}}_{L} {\mathbf {x}}_{N} + b_{L})\\ \end{bmatrix}\!, \\ {\boldsymbol {\beta }}=&amp;\left ({ {\boldsymbol {\beta }}_{1}^{T} \cdots {\boldsymbol {\beta }} _{L}^{T} }\right )^{T}\!,~ {\mathbf {T}}= \left ({ {\mathbf {y}}_{1}^{T} \cdots {\mathbf {y}} _{N}^{T} }\right )^{T}\!\!. \end{align}</span></span></p>
<div class="figure figure-full" id="fig_2">
<div class="img-wrap">
<a href="/ielx7/6287639/7042252/7140733/html/img/7140733-fig-2-large.gif"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-fig-2-small.gif" width="550" height="449" alt="Figure 2" /></a>
</div>
<div class="figcaption">
<b class="title">Figure 2.</b> A matrix form of an ELM.<!--&#x00A9; 2015 IEEE--></div>
<p class="links"><a href="#fig_1" class="prev">Previous</a> | <a href="icp.jsp?arnumber=7140733&pgName=figures" class="all">View All</a> | <a href="#fig_3" class="next">Next</a></p>
</div>
<p>Although the ELM procedure includes a training aspect, like other neural networks, the network structure itself is not noticeable in practice. Mathematically, there is only a matrix describing the projection between the two linear spaces. Thus an ELM is viewed as two projections: input <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-27-small.png" width="33" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {X}} {\mathbf {W}} $</span></span> and output <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-28-small.png" width="26" height="16" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}} {\boldsymbol {\beta }} $</span></span>, with a (non-linear) transformation between them <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-29-small.png" width="121" height="18" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}= \phi ( {\mathbf {X}} {\mathbf {W}} + \mathbf {b})$</span></span>. The number of hidden neurons regulates the size of matrices <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-30-small.png" width="19" height="13" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {W}}$</span></span>, <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-31-small.png" width="14" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}$</span></span> and <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-32-small.png" width="11" height="16" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\boldsymbol {\beta }}$</span></span>; but the network neurons are never treated separately.</p>
<p>With different types of hidden neurons, the first projection and transformation are performed independently for each type of neurons. Then the resulted sub-matrices <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-33-small.png" width="20" height="15" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}_{1}$</span></span> are concatenated along the second dimension. For two types of hidden neurons:<span class="formula" id="deqn5"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqdisp-4-small.png" width="429" height="35" alt="Formula" /><span class="link">TeX Source</span><span class="tex">\begin{equation} {\mathbf {H}}= [ {\mathbf {H}}_{1} ~| ~{\mathbf {H}}_{2}] = [\phi _{1}( {\mathbf {X}} {\mathbf {W}} _{1} + \mathbf {b}_{1}) ~| ~\phi _{2}( {\mathbf {X}} {\mathbf {W}} _{2} + \mathbf {b}_{2})].\qquad \end{equation}</span></span></p>
<p>Linear neurons are added into ELM by simply copying inputs into the hidden layer outputs:<span class="formula" id="deqn6"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqdisp-5-small.png" width="368" height="59" alt="Formula" /><span class="link">TeX Source</span><span class="tex">\begin{align} {\mathbf {H}}=&amp;[ {\mathbf {H}}_{1} ~| ~{\mathbf {H}}_{2} ~| ~{\mathbf {X}}]\notag \\=&amp;[\phi _{1}( {\mathbf {X}} {\mathbf {W}} _{1} + \mathbf {b}_{1}) ~| ~\phi _{2}( {\mathbf {X}} {\mathbf {W}} _{2} + \mathbf {b}_{2}) ~| ~{\mathbf {X}}]. \end{align}</span></span></p>
</div>
<div class="section_2" id="sec2d">
<h3>D. ELM Solution With Pseudo-Inverse</h3>
<p>Most often, an ELM problem is over-determined (<span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-34-small.png" width="48" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$N > L$</span></span>), with the number of training data samples larger than the number of hidden neurons.<a id="note-ref_3f"></a> For determined (<span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-35-small.png" width="48" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$N = L$</span></span>) and under-determined (<span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-36-small.png" width="48" height="11" alt="Formula" /><span class="html-formula"></span><span class="tex">$N &lt; L$</span></span>) instances, ELM should use regularization [<a href="#ref_3">3</a>]. Otherwise it has a poor generalization performance.</p>
<p>A unique solution for an over-determined system is given by a minimum <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-37-small.png" width="17" height="13" alt="Formula" /><span class="html-formula"></span><span class="tex">$L_{2}$</span></span> norm of the training error.<a id="note-ref_33a"></a> It may be found using the Moore-Penrose generalized inverse [<a href="#ref_33">33</a>] (pseudoinverse) of the matrix <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-38-small.png" width="14" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}$</span></span>, denoted as <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-39-small.png" width="20" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}^\dagger $</span></span>. As the matrix <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-40-small.png" width="14" height="12" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}$</span></span> has a full column rank, the pseudoinverse is computed as in <a href="#deqn7-9">equation (9)</a>.<span class="formula" id="deqn7-9"><img src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqdisp-6-small.png" width="292" height="79" alt="Formula" /><span class="link">TeX Source</span><span class="tex">\begin{align} {\mathbf {H}} {\boldsymbol {\beta }}=&amp;{\mathbf {T}} \\ {\boldsymbol {\beta }}=&amp;{\mathbf {H}}^\dagger {\mathbf {T}} \\ {\mathbf {H}}^\dagger=&amp;( {\mathbf {H}}^{T} {\mathbf {H}})^{-1} {\mathbf {H}}^{T}, \end{align}</span></span></p>
<p>The pseudoinverse is prone to numerical instabilities if the matrix <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-41-small.png" width="38" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}^{T} {\mathbf {H}}$</span></span> is close to singular. Practically (in Matlab<sup>&#x00AE;</sup> and Python), the implementations of the pseudoinverse include a small regularization term <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-42-small.png" width="155" height="18" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}^\dagger = ( {\mathbf {H}}^{T} {\mathbf {H}}+ \alpha {\mathbf {I}} ) {\mathbf {H}}^{T}$</span></span> where <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-43-small.png" width="55" height="13" alt="Formula" /><span class="html-formula"></span><span class="tex">$\alpha = 50\epsilon $</span></span> and <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-44-small.png" width="7" height="7" alt="Formula" /><span class="html-formula"></span><span class="tex">$\epsilon $</span></span> is the machine precision for a used type of floating point numbers. Adding a regularization term makes matrix <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-45-small.png" width="38" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$ {\mathbf {H}}^{T} {\mathbf {H}}$</span></span> non-singular, and the same solution applicable also for determined and under-determined systems.</p>
</div>
<div class="section_2" id="sec2e">
<h3>E. Classification With ELMs</h3>
<p>An ELM is a regression model, but it is easily adapted for classification. To classify a dataset with ELM, data targets need to be set in a special encoding manner.</p>
<p>If the classes are categorical and independent, then one target is created for each class. Targets for the correct classes are set to one, and targets for irrelevant classes are set to zero. This encoding creates a unit length vector for each class, which is orthogonal to vectors of all other classes. Distances between target vectors of different classes are the same, so the class independence is kept. The predicted class is assigned according to the target with the largest ELM output.</p>
<p>If the classes are ordinal and have a ranking, then they are translated into real numbers. Only one target is created for all the classes, and a predicted class is the one with the closest number to an ELM output.</p>
<p>In a multi-label problem, a sample can have multiple correct classes. The targets are created similarly as in the independent classes problem formulation. The predicted classes are assigned for all ELM outputs greater than a threshold value.</p>
<p>Using ELM for classification with independent classes changes the way how the prediction error is calculated. The classification error does not penalize (or encourage) small changes in the ELM output values, which do not lead to a different classification. This makes a difference in the model structure selection (described in <a href="#sec2f">section II-F</a>), where an optimization w.r.t. the MSE regression error finds an incorrect optimal number of hidden neurons, and creates a model with a sub-optimal classification prediction performance.</p>
</div>
<div class="section_2" id="sec2f">
<h3>F. Model Structure Selection in ELMs</h3>
<p>Model structure selection prevents ELM from learning noise from data and over-fitting. It does so by artificially limiting the learning ability of an ELM. A training dataset has multiple instances of inputs, and the corresponding targets, which are generated by the projected data and an added noise. The <i>noise</i> term includes both random noise and projection from features not present in the inputs. Learning particular data samples with the associated noise is called over-fitting. An over-fitted ELM model has worse generalization performance (prediction performance on new data), which can be measured using a validation set of data. A model structure selection process finds an optimal generalization performance by changing the amount of model parameters or applying regularization to the model.</p>
<p>A hyper-parameter of ELMs, which governs the amount of effective parameters, is the number of hidden neurons. The optimum number of neurons is found with a validation set, a cross-validation procedure or a Leave-One-Out validation procedure (which has an efficient solution in ELMs). Hidden neurons can be added and removed randomly, or they can be ranked by their relevance to the problem.<a id="note-ref_28c"></a> This ranking is called &#x201C;Optimal Pruning&#x201D; [<a href="#ref_28">28</a>] and it achieves better performance with a trade-off of a longer runtime. Neuron pruning methods correspond to <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-46-small.png" width="16" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{1}$</span></span>-regularization.</p>
<p><a id="note-ref_34a"></a>Another model structure selection technique available in ELMs is the Tikhonov regularization [<a href="#ref_34">34</a>]. It reduces an effective number of model parameters by reducing the influence of neuron outputs without removing neurons by themselves. Tikhonov regularization is efficient for achieving numerical stability in near-singular ELMs (and linear problems in general).<a id="note-ref_29b"></a> This regularization corresponds to <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-47-small.png" width="17" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{2}$</span></span>-regularization, and can be combined with <span class="inline-formula"><img align="bottom" src="/ielx7/6287639/7042252/7140733/html/img/7140733-eqin-48-small.png" width="16" height="14" alt="Formula" /><span class="html-formula"></span><span class="tex">$L^{1}$</span></span> to achieve the best results [<a href="#ref_29">29</a>].</p>
<p>Model structure selection is less important in Big Data tasks, because with a large number of samples a model learns to ignore noise. Large tasks are often complex enough not to overfit even at the limits of the hardware. Also, most model structure selection methods significantly increase runtime, which is a limiting factor for training large ELM models. For the provided reasons, only one fast neuron pruning method with a validation set is included in the toolbox part for large data.</p>
</div>
</div>
<h1>End of reading sample<h1>
<a href="read2.html" class="btn btn--m btn--full">Go to next reading<a>
</body>
</html>